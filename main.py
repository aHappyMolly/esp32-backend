# main.py â€”â€” å¯åˆ‡æ› provider çš„å¾Œç«¯éª¨æ¶
# è·¯ç”±ï¼š/sttï¼ˆèªéŸ³â†’æ–‡å­—ï¼‰ã€/chatï¼ˆæ–‡å­—â†’RAGâ†’LLMï¼‰ã€/health
# ç”¨æ³•ï¼šè¨­å®šç’°å¢ƒè®Šæ•¸æ±ºå®š STT / RAG / LLM ä¾›æ‡‰å•†ï¼Œç„¶å¾Œå•Ÿå‹•ï¼š
#   uvicorn main:app --host 0.0.0.0 --port 8000 --reload

import os
import io
import glob
import json
import math
import tempfile
import datetime
import json, re, requests
import zipfile, time
from typing import List, Optional
from bs4 import BeautifulSoup
from pydantic import BaseModel
from fastapi import FastAPI, UploadFile, File, Request, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, Response, HTMLResponse, JSONResponse
from app.tools.router import run_tools  # â† æ–°å¢ import
from pathlib import Path
from app.rag.loaders import (
    read_text_file, extract_pdf_text, extract_docx_text, extract_url_text
)
from app.ui.rag_admin import mount_rag_admin
from urllib.parse import urljoin




# ============= ç’°å¢ƒåƒæ•¸ï¼ˆå¯åœ¨ PowerShell/cmd æˆ– .env è¨­å®šï¼‰ =============
# ä¾›æ‡‰å•†é¸é …ï¼š
#   STT:   "local"ï¼ˆfaster-whisperï¼‰| "openai"
#   RAG:   "local"ï¼ˆæ¥µç°¡æª¢ç´¢ï¼‰| "openai"ï¼ˆOpenAI Embeddingsï¼‰| "none"
#   LLM:   "openai"ï¼ˆå»ºè­°ï¼›æœ¬åœ° LLM ä¹‹å¾Œå¯å†æ“´å……ï¼‰
PROVIDER_STT  = os.getenv("PROVIDER_STT",  "local")     # local | openai
PROVIDER_RAG  = os.getenv("PROVIDER_RAG",  "none")      # local | openai | none
PROVIDER_LLM  = os.getenv("PROVIDER_LLM",  "openai")    # openai


# Whisperï¼ˆæœ¬åœ° STTï¼‰è¨­å®š
WHISPER_MODEL     = os.getenv("WHISPER_MODEL", "small") # tiny/base/small/medium/large-v3 ...
WHISPER_DEVICE    = os.getenv("WHISPER_DEVICE", "cuda") # cuda | cpu
WHISPER_COMPUTE   = os.getenv("WHISPER_COMPUTE", "float16")  # float16 | int8_float16 | float32

# OpenAI è¨­å®š
OPENAI_API_KEY    = os.getenv("OPENAI_API_KEY", "")
OPENAI_CHAT_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_EMBED_MODEL= os.getenv("EMBED_MODEL", "text-embedding-3-small")
OPENAI_STT_MODEL  = os.getenv("OPENAI_STT_MODEL", "whisper-1")  # æˆ– gpt-4o-mini-transcribe
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")

# RAG çŸ¥è­˜ä¾†æºï¼ˆæœƒè®€é€™å€‹è³‡æ–™å¤¾ä¸‹çš„ .txt/.md/.mdx/.html æª”ï¼‰
KNOWLEDGE_DIR     = os.getenv("KNOWLEDGE_DIR", "./knowledge")
RAG_TOP_K         = int(os.getenv("RAG_TOP_K", "4"))

# èªè¨€åå¥½ï¼ˆç¹é«”ï¼‰
LANG_HINT         = os.getenv("LANG_HINT", "zh")     # STT èªè¨€æç¤º
REPLY_LANG        = os.getenv("REPLY_LANG", "zh-TW") # LLM å›è¦†èªè¨€åå¥½

# TTS è¨­å®šï¼ˆNEWï¼‰
PROVIDER_TTS   = os.getenv("PROVIDER_TTS", "openai")  # openai | piper
OPENAI_TTS_MODEL = os.getenv("OPENAI_TTS_MODEL", "gpt-4o-mini-tts")  # OpenAI TTS å‹è™Ÿ
OPENAI_TTS_VOICE = os.getenv("OPENAI_TTS_VOICE", "alloy")            # è²éŸ³æ¨£å¼
PIPER_MODEL_PATH = os.getenv("PIPER_MODEL_PATH", "")  # ä¾‹ï¼š/models/zh_CN-piper-medium.onnx


# ============= FastAPI åˆå§‹åŒ–èˆ‡ CORS =============
app = FastAPI(title="ESP32 Voice Backend (Pluggable)", version="1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"]
)

# åªæ˜¯è¨»å†Šä¸€å€‹ /rag çš„ GET è·¯ç”± ( app å·²å­˜åœ¨ â†’ rag_admin)
mount_rag_admin(app)


# ============= Provider ä»‹é¢å®šç¾© =============
class STTProvider:
    def transcribe(self, audio_bytes: bytes) -> str:
        raise NotImplementedError

class RAGProvider:
    def retrieve(self, query: str, k: int = 4) -> List[str]:
        return []

class LLMProvider:
    def chat(self, user_text: str, context_chunks: List[str], lang: str = "zh-TW") -> str:
        raise NotImplementedError
    
class TTSProvider:
    def synth(self, text: str, sr: int = 16000) -> bytes:
        """å›å‚³æ•´æ®µ WAVï¼ˆbytesï¼‰ã€‚"""
        raise NotImplementedError
    



# ============= STT Providers =============
# A) local: faster-whisperï¼ˆGPU/CPUï¼‰
class LocalWhisperProvider(STTProvider):
    def __init__(self):
        from faster_whisper import WhisperModel
        self.model = WhisperModel(
            WHISPER_MODEL,
            device=WHISPER_DEVICE,         # "cuda" or "cpu"
            compute_type=WHISPER_COMPUTE   # "float16" ç­‰
        )

    def transcribe(self, audio_bytes: bytes) -> str:
        # faster-whisper æœ€ç©©åƒæª”æ¡ˆè·¯å¾‘ â†’ å…ˆè½åœ°æš«å­˜æª”
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp.write(audio_bytes)
            path = tmp.name
        segments, info = self.model.transcribe(
            path,
            language=LANG_HINT,
            vad_filter=True,
            temperature=0.0,
            beam_size=5,
            best_of=5,
            condition_on_previous_text=False,
            initial_prompt="è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡ºï¼Œä¿ç•™å°ç£ç”¨èªã€‚å°ˆæœ‰åè©ï¼šESP32ã€ä¼ºæœé¦¬é”ã€éº¥å…‹é¢¨ã€ç›¸æ©Ÿã€éŸŒé«”ã€OTAã€ä¸Šå‚³ä¸‹è¼‰ã€‚"
        )
        text = "".join(seg.text for seg in segments).strip()
        return text

# B) openai STT
class OpenAIWhisperProvider(STTProvider):
    def __init__(self):
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY missing for OpenAI STT")
        from openai import OpenAI
        self.client = OpenAI()


    def transcribe(self, audio_bytes: bytes) -> str:
        from tempfile import NamedTemporaryFile
        # OpenAI SDK éœ€è¦ file-likeï¼›é€™è£¡ä¹Ÿèµ°æš«å­˜æª”
        with NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp.write(audio_bytes)
            tmp_path = tmp.name
        # whisper-1ï¼šå‚³çµ± Whisperï¼›è‹¥ç”¨ gpt-4o-mini-transcribe éœ€ä¾å®˜æ–¹èªéŸ³APIæ›´æ–°
        r = self.client.audio.transcriptions.create(
            model=OPENAI_STT_MODEL,
            file=open(tmp_path, "rb"),
            # è‹¥è¦å¼·åˆ¶ zhï¼Œå¯ç”¨ language="zh"
            # language="zh",
            response_format="json"
        )
        # r.text å°‡æ˜¯æ–‡å­—
        return r.text.strip()


# ============= RAG Providers =============

def _load_documents_from_dir(path: str) -> List[str]:
    """è®€å– knowledge ä¸­çš„å„é¡æ–‡ä»¶ â†’ ç´”æ–‡å­— â†’ ç°¡å–®åˆ‡å¡Š"""
    chunks: List[str] = []
    exts = [".txt", ".md", ".mdx", ".html", ".htm", ".pdf", ".docx"]
    for p in Path(path).rglob("*"):
        if not p.is_file() or p.suffix.lower() not in exts:
            continue

        text = None
        suf = p.suffix.lower()
        if suf in [".txt", ".md", ".mdx", ".html", ".htm"]:
            text = read_text_file(p)
        elif suf == ".pdf":
            text = extract_pdf_text(p)
        elif suf == ".docx":
            text = extract_docx_text(p)

        if not (text and text.strip()):
            continue

        # --- ä½ çš„åŸæœ¬åˆ‡å¡Šç­–ç•¥ï¼ˆæ¯å¡Š â‰¤800 å­—ï¼‰ ---
        for para in text.splitlines():
            t = (para or "").strip()
            if not t:
                continue
            while len(t) > 800:
                chunks.append(t[:800])
                t = t[800:]
            if t:
                chunks.append(t)
    return chunks

    # --- è‹¥æœªä¾†è¦ OCR æƒæç‰ˆ PDFï¼šåœ¨ extract_pdf_text ä¸­åµæ¸¬ page.imagesï¼Œå¿…è¦æ™‚åˆ‡åœ–ä¸Ÿ pytesseract


# --- åœ–ç‰‡å»ºè­°ï¼šå¾å·¥å…·ä¾†æº URL æŠ“ og:image + å¾ knowledge/.meta.json æŒ‘é—œéµåœ– ---
# å³ä½¿ç›®å‰çš„ RAG chunk æ²’å¸¶ã€Œä¾†æºæª”åã€ï¼Œä¹Ÿå¯å…ˆç”¨ã€Œå·¥å…·ä¾†æº URLã€èˆ‡ã€Œknowledge çš„ .meta.jsonã€ä¾†æŒ‘åœ–
def _images_from_tool_sources(sources, max_n=3):
    imgs = []
    for s in sources or []:
        url = (s.get("url") if isinstance(s, dict) else None) or ""
        if not url or not url.startswith("http"): 
            continue
        try:
            r = requests.get(url, timeout=6, headers={"User-Agent":"Mozilla/5.0"})
            html = r.text
            # og:image
            m = re.search(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.I)
            if m:
                img = m.group(1).strip()
                if not img.startswith("http"):
                    img = urljoin(url, img)
                imgs.append(img)
        except Exception:
            pass
        if len(imgs) >= max_n:
            break
    return imgs

# å–ä»£ main.py å…§çš„ _images_from_meta_by_query
def _images_from_meta_by_query(query: str, max_n=3):
    """æƒæ knowledge/*.meta.jsonï¼Œè‹¥ title/url/tags æœ‰ä»»ä½• token å‘½ä¸­ï¼Œå°±æ”¶é›† image/imagesã€‚"""
    def _tokens(s: str):
        s = (s or "").lower().strip()
        out, buf = [], []
        for ch in s:
            # ä¸­æ–‡ï¼šé€å­—ï¼›è‹±æ–‡æ•¸å­—ï¼šèšæˆè©
            if "\u4e00" <= ch <= "\u9fff":
                if buf: out.append("".join(buf)); buf=[]
                out.append(ch)
            elif ch.isalnum():
                buf.append(ch)
            else:
                if buf: out.append("".join(buf)); buf=[]
        if buf: out.append("".join(buf))
        # éæ¿¾å¤ªçŸ­çš„è‹±æ–‡ token
        return [t for t in out if (len(t)>=2 or ("\u4e00"<=t<= "\u9fff"))]

    q_tokens = _tokens(query)
    if not q_tokens:
        return []

    out = []
    root = Path(KNOWLEDGE_DIR)
    for m in root.glob("*.meta.json"):
        try:
            meta = json.loads(m.read_text(encoding="utf-8"))
        except Exception:
            continue

        hay = " ".join([
            str(meta.get("title","")),
            str(meta.get("url","")),
            " ".join(meta.get("tags",[])),
        ]).lower()

        # åªè¦ä»»ä¸€ token å‡ºç¾åœ¨ title/url/tags å°±è¦–ç‚ºå‘½ä¸­
        if any(tok in hay for tok in q_tokens):
            if meta.get("image"):
                out.append(meta["image"])
            if isinstance(meta.get("images"), list):
                out.extend([x for x in meta["images"] if isinstance(x, str)])

        if len(out) >= max_n:
            break

    # å»é‡ã€é™é‡
    seen, uniq = set(), []
    for u in out:
        if u not in seen:
            seen.add(u); uniq.append(u)
    return uniq[:max_n]




# A) local RAGï¼ˆå…å¤–éƒ¨ä¾è³´çš„æ¥µç°¡æª¢ç´¢ï¼štf-idf-like + overlap åˆ†æ•¸ï¼‰
class LocalNaiveRAG(RAGProvider):
    def __init__(self, knowledge_dir: str):
        self.chunks = _load_documents_from_dir(knowledge_dir)
        # å»ºä¸€äº›è¼•é‡ç´¢å¼•ï¼ˆword -> dfï¼‰
        self.N = len(self.chunks)
        self.df = {}
        for c in self.chunks:
            tokens = set(self._tokenize(c))
            for t in tokens:
                self.df[t] = self.df.get(t, 0) + 1

    def _tokenize(self, s: str) -> List[str]:
        # æ¥µç°¡ tokenï¼ˆä¸­æ–‡ï¼šé€å­—ï¼›è‹±æ–‡ï¼šå°å¯«æ‹†è©ï¼‰ï¼›å¯æ› jieba ç­‰
        out = []
        buf = []
        for ch in s:
            if "\u4e00" <= ch <= "\u9fff":
                out.append(ch)
            elif ch.isalnum():
                buf.append(ch.lower())
            else:
                if buf:
                    out.append("".join(buf))
                    buf = []
        if buf: out.append("".join(buf))
        return out

    def retrieve(self, query: str, k: int = 4) -> List[str]:
        if not self.chunks:
            return []
        q_tokens = self._tokenize(query)
        # tf-idf likeï¼šsum( tf * idf ) + å­—å…ƒé‡ç–Š bonus
        def score(text: str) -> float:
            tokens = self._tokenize(text)
            tf = {}
            for t in tokens:
                tf[t] = tf.get(t, 0) + 1
            s = 0.0
            for t in q_tokens:
                idf = math.log((self.N + 1) / (1 + self.df.get(t, 0) )) + 1.0
                s += tf.get(t, 0) * idf
            # åŠ ä¸€é»å­—å…ƒé‡ç–Šåº¦
            overlap = len(set(query) & set(text)) / (len(set(query)) + 1e-6)
            return s + overlap
        ranked = sorted(self.chunks, key=score, reverse=True)
        return ranked[:k]

# B) openai RAGï¼ˆç”¨ OpenAI Embeddings åšç°¡å–®å‘é‡æª¢ç´¢ï¼›å‘é‡å­˜è¨˜æ†¶é«”ï¼‰
class OpenAIRAG(RAGProvider):
    def __init__(self, knowledge_dir: str):
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY missing for OpenAI RAG")
        from openai import OpenAI
        self.client = OpenAI()
        self.model = OPENAI_EMBED_MODEL
        self.chunks = _load_documents_from_dir(knowledge_dir)
        self.vecs = self._embed_batch(self.chunks) if self.chunks else []

    def _embed_batch(self, texts: List[str]) -> List[List[float]]:
        # OpenAI embeddingsï¼šä¸€æ¬¡å¯é€å¤šç­†
        r = self.client.embeddings.create(model=self.model, input=texts)
        return [d.embedding for d in r.data]

    def _cos(self, a: List[float], b: List[float]) -> float:
        if not a or not b: return 0.0
        s = sum(x*y for x,y in zip(a,b))
        na = math.sqrt(sum(x*x for x in a))
        nb = math.sqrt(sum(y*y for y in b))
        return s / (na*nb + 1e-9)

    def retrieve(self, query: str, k: int = 4) -> List[str]:
        if not self.chunks:
            return []
        qv = self._embed_batch([query])[0]
        scored = [(self._cos(qv, v), i) for i, v in enumerate(self.vecs)]
        scored.sort(reverse=True)
        top = [self.chunks[i] for _, i in scored[:k]]
        return top


# ============= LLM Providerï¼ˆOpenAIï¼‰ =============
class OpenAILLM(LLMProvider):
    def __init__(self):
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY missing for OpenAI LLM")
        from openai import OpenAI
        self.client = OpenAI()
        self.model  = OPENAI_CHAT_MODEL

    def chat(self, user_text: str, context_chunks: List[str], lang: str = "zh-TW") -> str:
        system_prompt = (
            "ä½ æ˜¯ä¸€ä½èªéŸ³åŠ©ç†ï¼Œæ‰€æœ‰å›è¦†ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆè‡ºç£ï¼‰ï¼Œå£èªã€ç°¡çŸ­ã€ç›´è¦ºã€‚"
            "è‹¥æœ‰é™„å¸¶çš„åƒè€ƒå…§å®¹ï¼Œè«‹ç›¡é‡å¼•ç”¨å…¶ä¸­è³‡è¨Šå›ç­”ã€‚"
        )
        context_block = ""
        if context_chunks:
            joined = "\n\n---\n".join(context_chunks)
            context_block = f"ä»¥ä¸‹æ˜¯å¯åƒè€ƒçš„å…§å®¹ç‰‡æ®µï¼Œè«‹åœ¨åˆé©æ™‚å¼•ç”¨ï¼š\n{joined}\n\n"

        msgs = [
            {"role": "system", "content": system_prompt},
            {"role": "user",   "content": context_block + f"\nä½¿ç”¨è€…èªªï¼š{user_text}"}
        ]
        r = self.client.chat.completions.create(
            model=self.model,
            messages=msgs,
            temperature=0.5,
            max_tokens=300,
        )
        return r.choices[0].message.content.strip()
    

# ============= TTS Providerï¼ˆNEWï¼‰ =============
# A) OpenAI TTSï¼ˆé›²ç«¯ï¼Œæœ€ç°¡å–®ï¼‰

class OpenAITTSProvider(TTSProvider):
    def __init__(self):
        if not OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY missing for OpenAI TTS")
        import requests  # ç¢ºä¿ requirements.txt æœ‰ requests
        self.requests = requests
        self.model  = OPENAI_TTS_MODEL or "gpt-4o-mini-tts"
        self.voice  = OPENAI_TTS_VOICE or "alloy"
        self.base   = "https://api.openai.com/v1/audio/speech"
        self.headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json",
        }

    def synth(self, text: str, sr: int = 16000) -> bytes:
        payload = {"model": self.model, "voice": self.voice, "input": text, "format": "wav", "sample_rate": int(sr)}
        r = self.requests.post(self.base, headers=self.headers, json=payload, timeout=60)
        if r.status_code == 400:
            # æŸäº›æ¨¡å‹/ç‰ˆæœ¬ä¸æ¥å— sample_rateï¼Œç§»é™¤å¾Œé‡è©¦
            payload.pop("sample_rate", None)
            r = self.requests.post(self.base, headers=self.headers, json=payload, timeout=60)
        if r.status_code != 200:
            raise RuntimeError(f"OpenAI TTS HTTP {r.status_code}: {r.text[:200]}")
        b = r.content
        if not (len(b) >= 12 and b[:4] == b"RIFF" and b[8:12] == b"WAVE"):
            raise RuntimeError("TTS returned non-WAV")
        return b





# B) Piperï¼ˆæœ¬åœ°ï¼Œé›¢ç·šï¼‰
class PiperTTSProvider(TTSProvider):
    def __init__(self):
        if not PIPER_MODEL_PATH or not os.path.exists(PIPER_MODEL_PATH):
            raise RuntimeError("Piper model not found; set PIPER_MODEL_PATH")
        # ä¾ä½ å®‰è£çš„å°è£è€Œå®šï¼Œä»¥ä¸‹ç¤ºæ„å¸¸è¦‹ API
        # pip install piper-phonemize piper-tts æˆ–ç›¸å®¹å¥—ä»¶
        from piper import PiperVoice
        self.voice = PiperVoice.load(PIPER_MODEL_PATH)

    def synth(self, text: str, sr: int = 16000) -> bytes:
        # Piper é è¨­æœƒå› PCM/æˆ–ç›´æ¥å¯«æª”ï¼›é€™è£¡å°‡å…¶åŒ…æˆ WAV
        import wave, struct
        import numpy as np
        # ç”¢ç”Ÿ 16-bit mono PCMï¼ˆnumpy int16ï¼‰
        pcm = self.voice.synthesize(text, length_scale=1.0)  # å–å¾— float32 PCMï¼ˆä¾å¥—ä»¶ç‰ˆæœ¬ä¸åŒï¼‰
        if pcm.dtype != np.int16:
            # è½‰ 16-bitï¼ˆç°¡å–®é™å¹…ï¼‰
            x = np.clip(pcm, -1.0, 1.0)
            pcm16 = (x * 32767.0).astype(np.int16)
        else:
            pcm16 = pcm

        # å°è£ WAVï¼ˆ16-bit monoï¼‰
        buf = io.BytesIO()
        with wave.open(buf, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(sr)
            wf.writeframes(pcm16.tobytes())
        return buf.getvalue()


# ============= åŠ å…¥ä¸€å€‹ç”¢ç”Ÿ beep WAV çš„å°å·¥å…· =============
# --- fallback: ç”¢ç”Ÿä¸€æ®µ 0.8 ç§’ 1kHz çš„ 16-bit mono WAV ---
def make_beep_wav(sr: int = 16000, dur_s: float = 0.8, freq: float = 1000.0) -> bytes:
    import math, io, wave, struct
    n = int(sr * dur_s)
    amp = 0.4
    buf = io.BytesIO()
    with wave.open(buf, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sr)
        for i in range(n):
            v = int(amp * 32767 * math.sin(2*math.pi*freq*i/sr))
            wf.writeframes(struct.pack("<h", v))
    return buf.getvalue()



# =============== RAG ä¸Šå‚³ç›¸é—œ ================

# é‡å»ºç´¢å¼• ( è®“ç›¸é—œ API å¯ä»¥å‘¼å«å®ƒé‡æ•´ RAG)
def _reload_rag_provider():
    """é‡æ–°è¼‰å…¥ RAGï¼ˆä¾ç…§ç›®å‰ PROVIDER_RAGï¼‰ï¼Œä»¥ä¾¿ä¸Šå‚³/æ–°å¢ç¶²å€å¾Œç«‹å³ç”Ÿæ•ˆã€‚"""
    global rag_provider
    if PROVIDER_RAG == "local":
        rag_provider = LocalNaiveRAG(KNOWLEDGE_DIR)
    elif PROVIDER_RAG == "openai":
        rag_provider = OpenAIRAG(KNOWLEDGE_DIR)
    elif PROVIDER_RAG == "none":
        rag_provider = RAGProvider()


# åˆ—å‡º knowledge ç›®éŒ„æª”æ¡ˆï¼ˆçµ¦ /rag/list ç”¨ï¼‰
def _list_knowledge_files():
    root = Path(KNOWLEDGE_DIR)
    root.mkdir(parents=True, exist_ok=True)
    items = []
    for p in sorted(root.glob("*")):
        if p.is_file():
            st = p.stat()
            items.append({
                "name": p.name,
                "size": st.st_size,
                "mtime": int(st.st_mtime),
                "ext": p.suffix.lower(),
            })
    return items



# ============= Provider è¼‰å…¥å™¨ =============
stt_provider: Optional[STTProvider] = None
rag_provider: Optional[RAGProvider] = None
llm_provider: Optional[LLMProvider] = None
tts_provider: Optional[TTSProvider] = None

@app.on_event("startup")
def load_providers():
    global stt_provider, rag_provider, llm_provider, tts_provider

    # STT
    if PROVIDER_STT == "local":
        stt_provider = LocalWhisperProvider()
    elif PROVIDER_STT == "openai":
        stt_provider = OpenAIWhisperProvider()
    else:
        raise RuntimeError(f"Unknown PROVIDER_STT: {PROVIDER_STT}")

    # RAG
    if PROVIDER_RAG == "local":
        rag_provider = LocalNaiveRAG(KNOWLEDGE_DIR)
    elif PROVIDER_RAG == "openai":
        rag_provider = OpenAIRAG(KNOWLEDGE_DIR)
    elif PROVIDER_RAG == "none":
        rag_provider = RAGProvider()  # ç©ºå¯¦ä½œï¼Œretrieve() å› []
    else:
        raise RuntimeError(f"Unknown PROVIDER_RAG: {PROVIDER_RAG}")

    # LLM
    if PROVIDER_LLM == "openai":
        llm_provider = OpenAILLM()
    else:
        raise RuntimeError(f"Unknown PROVIDER_LLM: {PROVIDER_LLM}")

    # TTSï¼ˆNEWï¼‰
    if PROVIDER_TTS == "openai":
        tts_provider = OpenAITTSProvider()
    elif PROVIDER_TTS == "piper":
        tts_provider = PiperTTSProvider()
    else:
        raise RuntimeError(f"Unknown PROVIDER_TTS: {PROVIDER_TTS}")
    



# ============= è·¯ç”± =============
@app.get("/health")
def health():
    return {
        "ok": True,
        "stt": PROVIDER_STT,
        "rag": PROVIDER_RAG,
        "llm": PROVIDER_LLM,
        "tts": dict(
            provider=PROVIDER_TTS,
            openai=dict(model=OPENAI_TTS_MODEL, voice=OPENAI_TTS_VOICE, has_key=bool(OPENAI_API_KEY)),
            piper=dict(model_path=bool(PIPER_MODEL_PATH)),
        ),
        "whisper": dict(model=WHISPER_MODEL, device=WHISPER_DEVICE, compute=WHISPER_COMPUTE),
        "openai": dict(
            chat_model=OPENAI_CHAT_MODEL,
            embed_model=OPENAI_EMBED_MODEL,
            stt_model=OPENAI_STT_MODEL,
            has_key=bool(OPENAI_API_KEY),
        ),
        "knowledge_loaded": (isinstance(rag_provider, RAGProvider) and
                             hasattr(rag_provider, "chunks") and
                             len(getattr(rag_provider, "chunks", []))),
    }



# --- main.py è¿½åŠ ï¼šè¼•é‡é ç†± ---
from fastapi import BackgroundTasks
from fastapi.responses import JSONResponse

_warm_ready = False

def _do_warm():
    global _warm_ready
    try:
        # è§¸ç™¼ RAG/LLM/TTS ä¾›æ‡‰å™¨çš„ lazy initï¼ˆçš†ç‚ºè¼•é‡å‘¼å«ï¼‰
        try:
            _ = getattr(rag_provider, "retrieve", lambda q, k=1: [])("hi", 1)
        except Exception as e:
            print("[warm] rag:", e)
        try:
            if llm_provider:
                _ = llm_provider.chat("ping", [], lang="zh-TW")
        except Exception as e:
            print("[warm] llm:", e)
        # è‹¥ä½ æƒ³é€£ TTS ä¹Ÿé ç†±ï¼Œæ”¾é–‹ä¸‹é¢ 3 è¡Œå³å¯ï¼ˆæœƒç¨å¢å†·å•Ÿæ™‚å»¶ï¼‰
        # try:
        #     if tts_provider:
        #         _ = tts_provider.synth("hi", sr=16000)
        # except Exception as e:
        #     print("[warm] tts:", e)

        _warm_ready = True
    except Exception as e:
        print("[warm] error:", e)

@app.get("/warm")
def warm(tasks: BackgroundTasks):
    """éé˜»å¡é ç†±ï¼›è‹¥å°šæœª readyï¼Œå› 202 ä¸¦åœ¨èƒŒæ™¯è·‘ä¸€æ¬¡é ç†±ã€‚"""
    global _warm_ready
    if _warm_ready:
        return {"ok": True, "ready": True}
    tasks.add_task(_do_warm)
    return JSONResponse({"ok": True, "ready": False}, status_code=202)



# /sttï¼šæ”¯æ´ multipart "file" èˆ‡ raw bytesï¼ˆContent-Type: audio/wavï¼‰
@app.post("/stt")
async def stt(file: Optional[UploadFile] = File(None), request: Request = None):
    if stt_provider is None:
        raise HTTPException(500, "STT provider not loaded")
    if file is not None:
        audio_bytes = await file.read()
    else:
        audio_bytes = await request.body()
    if not audio_bytes:
        raise HTTPException(400, "No audio provided")
    try:
        text = stt_provider.transcribe(audio_bytes)
        return {"ok": True, "text": text}
    except Exception as e:
        raise HTTPException(500, f"STT error: {e}")


class ChatIn(BaseModel):
    text: str
    device_context: Optional[dict] = None   # â† å…è¨±å‰ç«¯é™„åº§æ¨™/æ™‚å€

@app.post("/chat")
def chat(body: ChatIn):
    if llm_provider is None or rag_provider is None:
        raise HTTPException(500, "Providers not loaded")
    try:
        tool_chunks, tool_sources = run_tools(body.text, body.device_context)  # â† æ–°å¢
        rag_chunks = rag_provider.retrieve(body.text, k=RAG_TOP_K) if hasattr(rag_provider, "retrieve") else []
        chunks = tool_chunks + rag_chunks

        reply  = llm_provider.chat(body.text, chunks, lang=REPLY_LANG)

        images_a = _images_from_tool_sources(tool_sources, max_n=3)
        images_b = _images_from_meta_by_query(body.text, max_n=3)
        images = (images_a + images_b)[:3]

        return {"ok": True, "reply": reply, "ctx_used": len(chunks), "sources": tool_sources, "images": images}

    except Exception as e:
        raise HTTPException(500, f"Chat error: {e}")
    



class TTSIn(BaseModel):
    text: str
    sr: Optional[int] = 16000

@app.post("/tts")
def tts(body: TTSIn):
    text = (body.text or "").strip()
    if not text:
        raise HTTPException(400, "empty text")
    sr = int(body.sr or 16000)

    # å…ˆé å‚™ä¸€å€‹ resp ç‰©ä»¶ï¼Œæ–¹ä¾¿é™„åŠ  header
    out_bytes: bytes = b""
    impl = "rest"
    err  = ""

    try:
        if tts_provider is None:
            impl = "beep"
            out_bytes = make_beep_wav(sr=sr, dur_s=0.8, freq=1000.0)
        else:
            out_bytes = tts_provider.synth(text, sr=sr)
    except Exception as e:
        impl = "beep"
        err  = str(e)
        out_bytes = make_beep_wav(sr=sr, dur_s=0.8, freq=600.0)

    resp = StreamingResponse(io.BytesIO(out_bytes), media_type="audio/wav")
    resp.headers["X-TTS-Impl"] = impl
    if err:
        resp.headers["X-TTS-Error"] = err[:300]
    return resp

# ï¼ˆå¯é¸ï¼‰GET /tts æ–¹ä¾¿ç€è¦½å™¨æ¸¬
@app.get("/tts")
def tts_get(text: Optional[str] = "", sr: Optional[int] = 16000):
    t = (text or "").strip() or "ç³»çµ±æ¸¬è©¦éŸ³"
    return tts(TTSIn(text=t, sr=sr))


@app.get("/tts_mp3")
def tts_mp3(text: str = "", voice: str = "alloy"):
    """å› MP3ï¼ˆaudio/mpegï¼‰ï¼Œçµ¦ ESP32-S3 + ESP8266Audio ç›´æ¥ä¸²æµæ’­æ”¾ã€‚"""
    t = (text or "").strip()
    if not t:
        raise HTTPException(400, "empty text")

    if not OPENAI_API_KEY:
        raise HTTPException(500, "OPENAI_API_KEY missing")

    import requests
    url = "https://api.openai.com/v1/audio/speech"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": OPENAI_TTS_MODEL or "gpt-4o-mini-tts",
        "voice": voice or (OPENAI_TTS_VOICE or "alloy"),
        "input": t,
        "format": "mp3",          # â˜… è¦ MP3
        # MP3 è‡ªå¸¶æ¡æ¨£ç‡ï¼Œé€šå¸¸ä¸æŒ‡å®š sample_rate
    }

    r = requests.post(url, headers=headers, json=payload, timeout=60)
    if r.status_code != 200:
        raise HTTPException(500, f"TTS failed: {r.status_code} {r.text[:200]}")

    b = r.content
    # æ˜ç¢ºå‘ŠçŸ¥é•·åº¦èˆ‡é—œé–‰é€£ç·š â†’ ESP32 ä¸²æµæ›´ç©©
    return Response(
        content=b,
        media_type="audio/mpeg",
        headers={
            "Content-Length": str(len(b)),
            "Cache-Control": "no-store",
            "Connection": "close",
        },
    )



# æ–¹ä¾¿æŠŠè³‡æ–™ä¸Ÿé€² RAG ( ä¸Šå‚³æª”æ¡ˆ â†’ å­˜é€² knowledge â†’ é‡æ–°ç´¢å¼• )
@app.post("/rag/upload")
async def rag_upload(file: UploadFile = File(...)):
    """æ”¯æ´ .txt .md .html .pdf .docxï¼Œä¸Šå‚³å¾Œå­˜é€² knowledge/ åŸæª”åï¼Œä¸¦é‡å»ºç´¢å¼•ã€‚"""
    suf = Path(file.filename).suffix.lower()
    if suf not in [".txt", ".md", ".mdx", ".html", ".htm", ".pdf", ".docx"]:
        raise HTTPException(400, "Unsupported file type")

    # ä¿å­˜åˆ° knowledge/
    dst = Path(KNOWLEDGE_DIR) / Path(file.filename).name
    dst.parent.mkdir(parents=True, exist_ok=True)
    data = await file.read()
    dst.write_bytes(data)

    # é‡å»ºç´¢å¼•
    _reload_rag_provider()

    return {"ok": True, "saved": str(dst.name), "chunks": len(getattr(rag_provider, "chunks", []))}


# æ–¹ä¾¿æŠŠè³‡æ–™ä¸Ÿé€² RAG (åŠ å…¥ç¶²å€ â†’ è½‰æ–‡å­— â†’ å­˜æˆ .txt â†’ é‡æ–°ç´¢å¼•)
@app.post("/rag/add_url")
async def rag_add_url(url: str = Form(...)):
    """æŠŠç¶²å€ä¸»è¦å…§æ–‡æŠ“ä¸‹ä¾†ï¼Œå­˜æˆ .txt åˆ° knowledgeï¼Œä¸¦é‡å»ºç´¢å¼•ã€‚"""
    text = extract_url_text(url)
    if not text or not text.strip():
        # åŠ å¼·é™¤éŒ¯ï¼šåœ¨ä¼ºæœå™¨æ—¥èªŒå°ä¸€æ¬¡ï¼Œå‰ç«¯çœ‹åˆ° 400 ä½†ä½ å¯åœ¨å¾Œç«¯æŸ¥çœ‹æ˜¯å“ªå€‹ URL æŠ“ä¸åˆ°
        print(f"[rag_add_url] extract failed: {url}")
        raise HTTPException(400, "Failed to extract text from URL (no text)")
    safe = "".join(ch for ch in url if ch.isalnum() or ch in "-_").strip("-_")[:60] or "page"
    dst = Path(KNOWLEDGE_DIR) / f"{safe}.txt"
    dst.parent.mkdir(parents=True, exist_ok=True)
    dst.write_text(text, encoding="utf-8")

    # æ“·å– title èˆ‡ og:imageï¼Œå¯«åŒå meta æª”
    title = ""; og_image = ""
    try:
        r = requests.get(url, timeout=6, headers={"User-Agent":"Mozilla/5.0"})
        soup = BeautifulSoup(r.text, "html.parser")
        title = (soup.title.string or "").strip() if soup.title else ""
        og = soup.find("meta", property="og:image")
        if og and og.get("content"): og_image = og.get("content").strip()
    except Exception:
        pass

    meta = {"url": url, "title": title}
    if og_image: meta["image"] = og_image
    (Path(KNOWLEDGE_DIR) / f"{safe}.txt.meta.json").write_text(
        json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8"
    )


    _reload_rag_provider()
    return {"ok": True, "saved": dst.name, "chars": len(text), "chunks": len(getattr(rag_provider, "chunks", []))}



@app.get("/rag/list")
def rag_list():
    return {"ok": True, "files": _list_knowledge_files()}

@app.post("/rag/delete")
def rag_delete(name: str = Form(...)):
    """åˆªé™¤ knowledge å…§æŒ‡å®šæª”æ¡ˆï¼Œä¸¦é‡å»ºç´¢å¼•ã€‚"""
    root = Path(KNOWLEDGE_DIR).resolve()
    target = (root / name).resolve()
    if not str(target).startswith(str(root)) or not target.exists() or not target.is_file():
        raise HTTPException(400, "file not found")
    target.unlink()
    _reload_rag_provider()
    return {"ok": True, "deleted": name, "chunks": len(getattr(rag_provider, "chunks", []))}

@app.post("/rag/reindex")
def rag_reindex():
    """æ‰‹å‹•é‡å»ºç´¢å¼•ã€‚"""
    _reload_rag_provider()
    return {"ok": True, "chunks": len(getattr(rag_provider, "chunks", []))}


# RAG å¾ç®¡ç†é ä¸Šå‚³å¾Œï¼Œå†æ‰“åŒ…ä¸‹è¼‰ï¼ŒæŠŠç·šä¸Šå…§å®¹æ‹‰å›æœ¬æ©Ÿå‚™ä»½ 
@app.get("/rag/backup")
def rag_backup():
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as z:
        root = Path(KNOWLEDGE_DIR)
        for p in root.glob("*"):
            if p.is_file():
                z.write(p, arcname=p.name)
    buf.seek(0)
    fname = f"knowledge_backup_{int(time.time())}.zip"
    return StreamingResponse(buf, media_type="application/zip",
                             headers={"Content-Disposition": f'attachment; filename="{fname}"'})


# ============= é–‹ç™¼å•Ÿå‹•ï¼ˆç›´æ¥ python main.py æ™‚ï¼‰ =============
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)





# ==== âœ¨ Low-latency STT Streaming (Segmented over HTTP) ====
from fastapi import WebSocket, WebSocketDisconnect
from fastapi import Depends
from fastapi.responses import StreamingResponse
from collections import defaultdict
import uuid
import asyncio
import threading

# æœƒè©±æš«å­˜ï¼šsid -> { "buf": bytearray, "started": ts, "last_push": ts, "sse_queue": asyncio.Queue }
_STT_SESS = {}
_STT_LOCK = threading.Lock()

def _stt_new_session():
    sid = uuid.uuid4().hex[:12]
    with _STT_LOCK:
        _STT_SESS[sid] = dict(
            buf=bytearray(),
            sse_queue=asyncio.Queue(maxsize=50),
            closed=False
        )
    return sid

def _stt_get(sid):
    with _STT_LOCK:
        return _STT_SESS.get(sid)

def _stt_close(sid):
    with _STT_LOCK:
        s = _STT_SESS.get(sid)
        if s: s["closed"] = True

@app.post("/stt/begin")
def stt_begin():
    """
    å›å‚³ä¸€å€‹ sidï¼ŒESP32 ä¹‹å¾Œç”¨é€™å€‹ sid é€£çºŒä¸Šå‚³åˆ†ç‰‡ã€‚
    """
    sid = _stt_new_session()
    return {"ok": True, "sid": sid}

@app.post("/stt/seg")
async def stt_seg(request: Request, sid: str):
    """
    é€£çºŒä¸Šå‚³éŸ³è¨Šåˆ†ç‰‡ï¼ˆåŸæ¨£ PCM/WAV çš†å¯ï¼›å»ºè­°å°åŒ… 16000Hz * 0.2 ç§’ = 3200 å–æ¨£ -> 6400 bytesï¼‰
    é€™è£¡åšã€Œç²—ç²’åº¦å¢é‡è½‰å¯«ã€ï¼šæ¯ç´¯ç© ~0.8 ç§’å°±è·‘ä¸€æ¬¡ STTï¼Œä¸¦æŠŠ partial å‚³åˆ° SSEã€‚
    """
    s = _stt_get(sid)
    if not s: 
        return JSONResponse({"ok": False, "err": "bad sid"}, status_code=400)
    chunk = await request.body()
    s["buf"].extend(chunk)

    # ---- é–¾å€¼åˆ°å°±è·‘ä¸€æ¬¡ã€Œå¢é‡è¾¨è­˜ã€ä¸¦æ¨é€ partial ----
    # ç‚ºäº†ç°¡å–®ç©©å®šï¼Œé€™è£¡æ¯ 0.8 ç§’åšä¸€æ¬¡æ•´æ®µé‡è·‘ï¼ˆæ¨¡å‹å¿«å–ä¸åšï¼‰ã€‚
    # ä½ ç”¨ GPU çš„ faster-whisper small/medium æ™‚é€šå¸¸ä¹Ÿå¤ å¿«ï¼›è¦æ›´å³æ™‚å¯æ›æˆçœŸæ­£ streaming APIã€‚
    MIN_BYTES_PER_PUSH = 16000 * 2 * 8 // 10  # 0.8s @16kHz 16-bit mono â‰ˆ 25600 bytes
    if len(s["buf"]) >= MIN_BYTES_PER_PUSH and isinstance(stt_provider, LocalWhisperProvider):
        try:
            # å¯«æš«å­˜æª”çµ¦ faster-whisper
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                tmp.write(s["buf"])
                path = tmp.name
            # è·‘ä¸€æ¬¡è¾¨è­˜ï¼ˆå¸¶ vadï¼Œé¿å…å¤ªå¤šç©ºç™½ï¼‰
            txt = LocalWhisperProvider().transcribe(Path(path).read_bytes())
            # æ¨ SSEï¼ˆpartialï¼‰
            try:
                await s["sse_queue"].put(json.dumps({"partial": txt}))
            except Exception:
                pass
        except Exception as e:
            print("[stt_seg]", e)

    return {"ok": True, "bytes": len(chunk)}

@app.get("/stt/sse")
async def stt_sse(sid: str):
    """
    SSE: å°å‰ç«¯ï¼ˆä½ çš„ llm_panel é ï¼‰æ¨é€å³æ™‚å­—å¹•èˆ‡çµå°¾çµæœã€‚
    """
    s = _stt_get(sid)
    if not s:
        return JSONResponse({"ok": False, "err": "bad sid"}, status_code=400)

    async def eventgen():
        # å…ˆä¸Ÿä¸€å€‹ hello
        yield f"data: {json.dumps({'hello':'sse','sid':sid})}\n\n"
        while True:
            if s["closed"] and s["sse_queue"].empty():
                break
            try:
                item = await asyncio.wait_for(s["sse_queue"].get(), timeout=1.5)
                yield f"data: {item}\n\n"
            except asyncio.TimeoutError:
                continue
        yield f"data: {json.dumps({'bye': True})}\n\n"

    return StreamingResponse(eventgen(), media_type="text/event-stream")

@app.post("/stt/end")
async def stt_end(sid: str):
    """
    é—œé–‰æœƒè©±ï¼Œåšä¸€æ¬¡æœ€çµ‚è¾¨è­˜ï¼Œæ¨é€ final çµæœï¼Œå›å‚³æ–‡å­—ã€‚
    """
    s = _stt_get(sid)
    if not s:
        return JSONResponse({"ok": False, "err": "bad sid"}, status_code=400)

    # æœ€çµ‚ä¸€æ¬¡å®Œæ•´è¾¨è­˜ï¼ˆå¯è¼ƒç²¾ç¢ºï¼‰
    try:
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp.write(s["buf"])
            path = tmp.name
        if isinstance(stt_provider, LocalWhisperProvider):
            text = LocalWhisperProvider().transcribe(Path(path).read_bytes())
        else:
            # è‹¥ä½ ç”¨ OpenAI STTï¼šç›´æ¥ä¸Ÿæ•´æ®µ
            text = OpenAIWhisperProvider().transcribe(Path(path).read_bytes())
    except Exception as e:
        text = ""

    # æ¨ final åˆ° SSE
    try:
        await s["sse_queue"].put(json.dumps({"final": text}))
    except Exception:
        pass
    _stt_close(sid)

    # ğŸ“Œ é€™è£¡ç›´æ¥å¹«ä½ æ¥åˆ° LLMï¼ˆ/chatï¼‰â†’ çœä¸€æ¬¡å¾€è¿”
    reply = ""
    try:
        ctx = rag_provider.retrieve(text, k=RAG_TOP_K) if hasattr(rag_provider, "retrieve") else []
        reply = llm_provider.chat(text, ctx, lang=REPLY_LANG)
        # ä½ åŸæœ¬çš„ TTS æµç¨‹ç¶­æŒä¸è®Šï¼Œç”±å‰ç«¯/ESP32 è‡ªè¡Œå‘¼å« /i2s/say
    except Exception as e:
        print("[stt_end LLM]", e)

    return {"ok": True, "text": text, "reply": reply}

